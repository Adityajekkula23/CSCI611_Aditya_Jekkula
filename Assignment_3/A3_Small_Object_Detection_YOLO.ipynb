{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: Small Object Detection Using YOLO\n",
    "\n",
    "**Course:** Computer Vision  \n",
    "**Assignment:** A3 — Small Object Detection  \n",
    "**Model:** YOLOv8 (Ultralytics)  \n",
    "**Dataset:** LISA Traffic Sign Dataset (via Roboflow)  \n",
    "**Classes:** `stopSign`, `warning`, `pedestrianCrossing`, `signalAhead`\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a complete pipeline for small object detection using the YOLO (You Only Look Once) architecture. The goal is to detect traffic signs from images captured by a vehicle-mounted camera. Since traffic signs can appear small and distant in real-world driving footage, we evaluate a pre-trained YOLOv8 baseline, then fine-tune it on the LISA Traffic Sign Dataset with configurations optimized for small object detection.\n",
    "\n",
    "**Pipeline Summary:**\n",
    "1. Environment setup and dependency installation\n",
    "2. Dataset download and preparation (LISA via Roboflow)\n",
    "3. Baseline evaluation using pre-trained YOLOv8\n",
    "4. Fine-tuning with small-object-optimized configurations\n",
    "5. Post-training evaluation and comparison\n",
    "6. Inference on test images with visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "Install all required dependencies. This notebook is designed to run on Google Colab with a T4 GPU runtime.  \n",
    "**Runtime → Change runtime type → T4 GPU** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics roboflow -q\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Please enable GPU runtime for faster training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Dataset Download and Preparation\n",
    "\n",
    "We use the **LISA Traffic Sign Dataset** sourced from Roboflow Universe. The dataset contains approximately 9,800 images of real-world traffic signs captured from a vehicle-mounted camera — making it well-suited for small object detection research.\n",
    "\n",
    "**Selected Classes:**\n",
    "| Class ID | Class Name | Description |\n",
    "|----------|------------|-------------|\n",
    "| 0 | stopSign | Octagonal red stop signs |\n",
    "| 1 | warning | Diamond-shaped yellow warning signs |\n",
    "| 2 | pedestrianCrossing | Pedestrian crossing ahead signs |\n",
    "| 3 | signalAhead | Traffic signal ahead warning signs |\n",
    "\n",
    "The dataset is downloaded directly via the Roboflow API in YOLOv8 format (`.txt` annotations + `data.yaml` config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   PASTE YOUR ROBOFLOW DOWNLOAD CODE SNIPPET BELOW\n",
    "#   You can find this on the Roboflow dataset page:\n",
    "#   Download Dataset → YOLOv8 → Show Download Code\n",
    "# ============================================================\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# TODO: Replace with your Roboflow code snippet\n",
    "# rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "# project = rf.workspace(\"WORKSPACE\").project(\"PROJECT\")\n",
    "# version = project.version(1)\n",
    "# dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the dataset directory and config file\n",
    "# Roboflow typically downloads to a folder in /content/\n",
    "\n",
    "import glob\n",
    "\n",
    "yaml_files = glob.glob('/content/**/*.yaml', recursive=True)\n",
    "print(\"Found YAML files:\", yaml_files)\n",
    "\n",
    "# Set the dataset YAML path (update if needed)\n",
    "DATASET_YAML = yaml_files[0] if yaml_files else '/content/dataset/data.yaml'\n",
    "print(f\"Using dataset config: {DATASET_YAML}\")\n",
    "\n",
    "# Display dataset config\n",
    "with open(DATASET_YAML, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    print(\"\\nDataset Configuration:\")\n",
    "    print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Filter Dataset to 4 Target Classes\n",
    "\n",
    "The LISA dataset contains 40 classes. We filter annotations to retain only our 4 target classes and remap their IDs to 0–3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter annotations to keep only our 4 target classes\n",
    "\n",
    "TARGET_CLASSES = ['stopSign', 'warning', 'pedestrianCrossing', 'signalAhead']\n",
    "\n",
    "def get_class_mapping(yaml_path, target_classes):\n",
    "    \"\"\"Build a mapping from original class IDs to new sequential IDs.\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    original_names = config.get('names', [])\n",
    "    mapping = {}\n",
    "    for new_id, cls_name in enumerate(target_classes):\n",
    "        if cls_name in original_names:\n",
    "            old_id = original_names.index(cls_name)\n",
    "            mapping[old_id] = new_id\n",
    "            print(f\"  '{cls_name}': original ID {old_id} → new ID {new_id}\")\n",
    "        else:\n",
    "            print(f\"  WARNING: '{cls_name}' not found in dataset classes\")\n",
    "    return mapping\n",
    "\n",
    "print(\"Class ID Remapping:\")\n",
    "class_mapping = get_class_mapping(DATASET_YAML, TARGET_CLASSES)\n",
    "\n",
    "def filter_annotation_file(label_path, class_mapping):\n",
    "    \"\"\"Filter a single annotation file to keep only target classes.\"\"\"\n",
    "    if not os.path.exists(label_path):\n",
    "        return 0\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    filtered = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        class_id = int(parts[0])\n",
    "        if class_id in class_mapping:\n",
    "            parts[0] = str(class_mapping[class_id])\n",
    "            filtered.append(' '.join(parts))\n",
    "    with open(label_path, 'w') as f:\n",
    "        f.write('\\n'.join(filtered))\n",
    "    return len(filtered)\n",
    "\n",
    "def filter_dataset(yaml_path, class_mapping):\n",
    "    \"\"\"Filter all annotation files in train/val/test splits.\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    dataset_root = str(Path(yaml_path).parent)\n",
    "    total_annotations = 0\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = config.get(split, '')\n",
    "        if not split_path:\n",
    "            continue\n",
    "        if not os.path.isabs(split_path):\n",
    "            split_path = os.path.join(dataset_root, split_path)\n",
    "        labels_path = split_path.replace('/images', '/labels')\n",
    "        if not os.path.exists(labels_path):\n",
    "            labels_path = os.path.join(dataset_root, split, 'labels')\n",
    "        if os.path.exists(labels_path):\n",
    "            label_files = list(Path(labels_path).glob('*.txt'))\n",
    "            count = sum(filter_annotation_file(str(lf), class_mapping) for lf in label_files)\n",
    "            total_annotations += count\n",
    "            print(f\"  {split}: processed {len(label_files)} files, kept {count} annotations\")\n",
    "    return total_annotations\n",
    "\n",
    "print(\"\\nFiltering dataset annotations...\")\n",
    "total = filter_dataset(DATASET_YAML, class_mapping)\n",
    "print(f\"\\nTotal annotations kept: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data.yaml to reflect our 4 classes\n",
    "\n",
    "with open(DATASET_YAML, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['names'] = TARGET_CLASSES\n",
    "config['nc'] = len(TARGET_CLASSES)\n",
    "\n",
    "with open(DATASET_YAML, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Updated data.yaml:\")\n",
    "with open(DATASET_YAML, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count class distribution across the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count_class_distribution(yaml_path, target_classes):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    dataset_root = str(Path(yaml_path).parent)\n",
    "    class_counts = Counter()\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = config.get(split, '')\n",
    "        if not split_path:\n",
    "            continue\n",
    "        if not os.path.isabs(split_path):\n",
    "            split_path = os.path.join(dataset_root, split_path)\n",
    "        labels_path = split_path.replace('/images', '/labels')\n",
    "        if not os.path.exists(labels_path):\n",
    "            labels_path = os.path.join(dataset_root, split, 'labels')\n",
    "        if os.path.exists(labels_path):\n",
    "            for lf in Path(labels_path).glob('*.txt'):\n",
    "                with open(lf) as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts:\n",
    "                            class_counts[int(parts[0])] += 1\n",
    "    return class_counts\n",
    "\n",
    "counts = count_class_distribution(DATASET_YAML, TARGET_CLASSES)\n",
    "\n",
    "# Plot class distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "class_names = [TARGET_CLASSES[i] for i in sorted(counts.keys())]\n",
    "values = [counts[i] for i in sorted(counts.keys())]\n",
    "bars = ax.bar(class_names, values, color=['#e74c3c', '#f39c12', '#3498db', '#2ecc71'], edgecolor='black', linewidth=0.8)\n",
    "ax.set_title('Class Distribution in Filtered LISA Dataset', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Traffic Sign Class', fontsize=11)\n",
    "ax.set_ylabel('Number of Annotations', fontsize=11)\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, str(val),\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nTotal annotations: {sum(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Test Split\n",
    "\n",
    "We randomly select 10–15% of images as a held-out test set for evaluating both the pre-trained and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify test split exists or create one from validation set\n",
    "\n",
    "with open(DATASET_YAML, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset_root = str(Path(DATASET_YAML).parent)\n",
    "\n",
    "# Check if test split is defined\n",
    "test_path = config.get('test', None)\n",
    "if test_path:\n",
    "    if not os.path.isabs(test_path):\n",
    "        test_path = os.path.join(dataset_root, test_path)\n",
    "    test_images = list(Path(test_path).glob('*.jpg')) + list(Path(test_path).glob('*.png'))\n",
    "    print(f\"Test split found: {len(test_images)} images at {test_path}\")\n",
    "else:\n",
    "    print(\"No test split found. Creating one from validation set (15%)...\")\n",
    "    val_path = config.get('val', 'valid/images')\n",
    "    if not os.path.isabs(val_path):\n",
    "        val_path = os.path.join(dataset_root, val_path)\n",
    "    val_images = list(Path(val_path).glob('*.jpg')) + list(Path(val_path).glob('*.png'))\n",
    "    n_test = max(1, int(len(val_images) * 0.15))\n",
    "    test_imgs = random.sample(val_images, n_test)\n",
    "\n",
    "    test_img_dir = os.path.join(dataset_root, 'test', 'images')\n",
    "    test_lbl_dir = os.path.join(dataset_root, 'test', 'labels')\n",
    "    os.makedirs(test_img_dir, exist_ok=True)\n",
    "    os.makedirs(test_lbl_dir, exist_ok=True)\n",
    "\n",
    "    for img_path in test_imgs:\n",
    "        shutil.copy(str(img_path), test_img_dir)\n",
    "        lbl_path = str(img_path).replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt'\n",
    "        if os.path.exists(lbl_path):\n",
    "            shutil.copy(lbl_path, test_lbl_dir)\n",
    "\n",
    "    config['test'] = 'test/images'\n",
    "    with open(DATASET_YAML, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    print(f\"Created test split: {n_test} images\")\n",
    "\n",
    "TEST_IMAGES_PATH = test_path if test_path else test_img_dir\n",
    "print(f\"Test images directory: {TEST_IMAGES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Baseline Evaluation — Pre-trained YOLOv8\n",
    "\n",
    "Before fine-tuning, we evaluate the performance of the stock pre-trained YOLOv8n model on our test set. This establishes a baseline for comparison after training.\n",
    "\n",
    "The pre-trained model was trained on the COCO dataset (80 classes) and has not seen LISA traffic sign data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLOv8n model (COCO weights)\n",
    "pretrained_model = YOLO('yolov8n.pt')\n",
    "print(\"Pre-trained YOLOv8n model loaded.\")\n",
    "print(f\"Number of classes (COCO): {pretrained_model.model.nc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images using pre-trained model\n",
    "print(\"Running baseline inference on test images...\")\n",
    "\n",
    "baseline_results = pretrained_model.predict(\n",
    "    source=TEST_IMAGES_PATH,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    save_txt=True,\n",
    "    project='/content/runs',\n",
    "    name='baseline_pretrained',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline inference complete.\")\n",
    "print(f\"Results saved to: /content/runs/baseline_pretrained/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline detection samples\n",
    "baseline_output_dir = '/content/runs/baseline_pretrained'\n",
    "output_images = list(Path(baseline_output_dir).glob('*.jpg')) + list(Path(baseline_output_dir).glob('*.png'))\n",
    "\n",
    "if output_images:\n",
    "    sample_imgs = random.sample(output_images, min(6, len(output_images)))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, img_path in enumerate(sample_imgs):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Baseline: {img_path.name}', fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "    for j in range(i+1, 6):\n",
    "        axes[j].axis('off')\n",
    "    fig.suptitle('Pre-trained YOLOv8 Baseline — Sample Detections on Test Images', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/baseline_samples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\nelse:\n",
    "    print(\"No output images found. Check the results directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count baseline detections above confidence threshold\n",
    "total_detections = 0\n",
    "images_with_detections = 0\n",
    "\n",
    "for result in baseline_results:\n",
    "    n = len(result.boxes) if result.boxes is not None else 0\n",
    "    total_detections += n\n",
    "    if n > 0:\n",
    "        images_with_detections += 1\n",
    "\n",
    "total_test_images = len(baseline_results)\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE (Pre-trained YOLOv8n) Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total test images:         {total_test_images}\")\n",
    "print(f\"Images with detections:    {images_with_detections}\")\n",
    "print(f\"Total detections (≥0.5):   {total_detections}\")\n",
    "print(f\"Detection rate:            {images_with_detections/total_test_images*100:.1f}%\")\n",
    "print(\"Note: Pre-trained model uses COCO classes, not LISA classes.\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Fine-tuning YOLOv8 for Small Object Detection\n",
    "\n",
    "We fine-tune YOLOv8n on the LISA dataset with the following configurations optimized for small object detection:\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| `imgsz` | 1280 | Higher resolution preserves detail of small/distant signs |\n",
    "| `epochs` | 30 | Sufficient convergence without overfitting |\n",
    "| `batch` | 8 | Reduced batch size to accommodate larger image resolution |\n",
    "| `augment` | True | Scale, flip, blur augmentations improve robustness |\n",
    "| `mosaic` | 1.0 | Mosaic augmentation exposes model to multi-scale objects |\n",
    "| `scale` | 0.5 | Random scaling simulates objects at varying distances |\n",
    "| `degrees` | 10.0 | Slight rotation augmentation for real-world variance |\n",
    "| `optimizer` | AdamW | Stable convergence for fine-tuning tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model from pre-trained weights for fine-tuning\n",
    "model = YOLO('yolov8n.pt')\n",
    "print(\"Model initialized from pre-trained YOLOv8n weights.\")\n",
    "print(\"Starting fine-tuning on LISA Traffic Sign Dataset...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "# Training may take 1-2 hours on a T4 GPU with imgsz=1280\n",
    "\n",
    "results = model.train(\n",
    "    data=DATASET_YAML,\n",
    "    epochs=30,\n",
    "    imgsz=1280,           # High resolution for small object detection\n",
    "    batch=8,              # Reduced batch size for larger resolution\n",
    "    augment=True,         # Enable built-in augmentations\n",
    "    mosaic=1.0,           # Mosaic augmentation (multi-scale)\n",
    "    scale=0.5,            # Random scale augmentation\n",
    "    degrees=10.0,         # Random rotation augmentation\n",
    "    fliplr=0.5,           # Horizontal flip probability\n",
    "    optimizer='AdamW',    # Optimizer\n",
    "    lr0=0.001,            # Initial learning rate\n",
    "    patience=10,          # Early stopping patience\n",
    "    save=True,\n",
    "    project='/content/runs',\n",
    "    name='lisa_finetuned',\n",
    "    exist_ok=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "BEST_MODEL_PATH = '/content/runs/lisa_finetuned/weights/best.pt'\n",
    "print(f\"Best model saved to: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves generated by Ultralytics\n",
    "import glob\n",
    "from IPython.display import display\n",
    "\n",
    "results_plot = '/content/runs/lisa_finetuned/results.png'\n",
    "if os.path.exists(results_plot):\n",
    "    img = Image.open(results_plot)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Training Metrics — Loss and mAP Curves', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\nelse:\n",
    "    print(\"Training results plot not found. Training may still be running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Post-Training Evaluation\n",
    "\n",
    "We evaluate the fine-tuned model on the held-out test set and compare performance metrics against the pre-trained baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best fine-tuned model\n",
    "finetuned_model = YOLO(BEST_MODEL_PATH)\n",
    "print(f\"Fine-tuned model loaded from: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model on test set\n",
    "print(\"Evaluating fine-tuned model on test set...\")\n",
    "\n",
    "finetuned_metrics = finetuned_model.val(\n",
    "    data=DATASET_YAML,\n",
    "    split='test',\n",
    "    conf=0.5,\n",
    "    iou=0.5,\n",
    "    project='/content/runs',\n",
    "    name='finetuned_eval',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuned Model Evaluation Metrics:\")\n",
    "print(f\"  mAP@0.50:       {finetuned_metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@0.50:0.95:  {finetuned_metrics.box.map:.4f}\")\n",
    "print(f\"  Precision:      {finetuned_metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall:         {finetuned_metrics.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images with fine-tuned model\n",
    "finetuned_results = finetuned_model.predict(\n",
    "    source=TEST_IMAGES_PATH,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    save_txt=True,\n",
    "    project='/content/runs',\n",
    "    name='finetuned_predictions',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Count fine-tuned detections\n",
    "ft_total_detections = 0\n",
    "ft_images_with_detections = 0\n",
    "for result in finetuned_results:\n",
    "    n = len(result.boxes) if result.boxes is not None else 0\n",
    "    ft_total_detections += n\n",
    "    if n > 0:\n",
    "        ft_images_with_detections += 1\n",
    "\n",
    "print(f\"Fine-tuned inference complete.\")\n",
    "print(f\"Total detections (≥0.5 conf): {ft_total_detections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of baseline vs fine-tuned\n",
    "\n",
    "map50_ft = finetuned_metrics.box.map50\n",
    "precision_ft = finetuned_metrics.box.mp\n",
    "recall_ft = finetuned_metrics.box.mr\n",
    "\n",
    "# Note: Pre-trained model cannot be evaluated with LISA classes (different class set)\n",
    "# We compare detection counts and qualitative results\n",
    "print(\"=\" * 60)\n",
    "print(\" MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Pre-trained':>12} {'Fine-tuned':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<30} {'YOLOv8n (COCO)':>12} {'YOLOv8n (LISA)':>12}\")\n",
    "print(f\"{'Classes':<30} {'80 (COCO)':>12} {'4 (LISA)':>12}\")\n",
    "print(f\"{'mAP@0.50':<30} {'N/A*':>12} {map50_ft:.4f:>12}\")\n",
    "print(f\"{'Precision':<30} {'N/A*':>12} {precision_ft:.4f:>12}\")\n",
    "print(f\"{'Recall':<30} {'N/A*':>12} {recall_ft:.4f:>12}\")\n",
    "print(f\"{'Detections on test set':<30} {total_detections:>12} {ft_total_detections:>12}\")\n",
    "print(f\"{'Images with detections':<30} {images_with_detections:>12} {ft_images_with_detections:>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"* Pre-trained model uses COCO classes; mAP cannot be\")\n",
    "print(\"  directly compared against LISA ground truth labels.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison of detections\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Detection counts\n",
    "axes[0].bar(['Pre-trained\\n(COCO)', 'Fine-tuned\\n(LISA)'],\n",
    "            [total_detections, ft_total_detections],\n",
    "            color=['#95a5a6', '#2ecc71'], edgecolor='black', linewidth=0.8)\n",
    "axes[0].set_title('Total Detections on Test Set\\n(confidence ≥ 0.5)', fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Detections')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([total_detections, ft_total_detections]):\n",
    "    axes[0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Fine-tuned model metrics\n",
    "metrics_names = ['mAP@0.50', 'Precision', 'Recall']\n",
    "metrics_values = [map50_ft, precision_ft, recall_ft]\n",
    "bars = axes[1].bar(metrics_names, metrics_values,\n",
    "                   color=['#3498db', '#e74c3c', '#f39c12'],\n",
    "                   edgecolor='black', linewidth=0.8)\n",
    "axes[1].set_title('Fine-tuned Model Performance Metrics\\n(evaluated on LISA test set)', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{val:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Pre-trained vs Fine-tuned YOLOv8 — Performance Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/comparison_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Inference Visualization and Analysis\n",
    "\n",
    "Visual comparison of detections from the pre-trained and fine-tuned models on the same test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display fine-tuned model detection samples\n",
    "ft_output_dir = '/content/runs/finetuned_predictions'\n",
    "ft_output_images = list(Path(ft_output_dir).glob('*.jpg')) + list(Path(ft_output_dir).glob('*.png'))\n",
    "\n",
    "if ft_output_images:\n",
    "    sample_imgs = random.sample(ft_output_images, min(6, len(ft_output_images)))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, img_path in enumerate(sample_imgs):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'{img_path.name}', fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "    for j in range(i+1, 6):\n",
    "        axes[j].axis('off')\n",
    "    fig.suptitle('Fine-tuned YOLOv8 (LISA) — Sample Detections on Test Images', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/finetuned_samples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\nelse:\n",
    "    print(\"No fine-tuned prediction images found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence score distribution for fine-tuned model\n",
    "all_confidences = []\n",
    "class_conf = {cls: [] for cls in TARGET_CLASSES}\n",
    "\n",
    "for result in finetuned_results:\n",
    "    if result.boxes is not None and len(result.boxes) > 0:\n",
    "        confs = result.boxes.conf.cpu().numpy()\n",
    "        cls_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "        for conf, cls_id in zip(confs, cls_ids):\n",
    "            all_confidences.append(conf)\n",
    "            if cls_id < len(TARGET_CLASSES):\n",
    "                class_conf[TARGET_CLASSES[cls_id]].append(conf)\n",
    "\n",
    "if all_confidences:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "    # Overall confidence histogram\n",
    "    axes[0].hist(all_confidences, bins=20, color='#3498db', edgecolor='black', linewidth=0.7)\n",
    "    axes[0].axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "    axes[0].set_title('Confidence Score Distribution\\n(All Detections)', fontweight='bold')\n",
    "    axes[0].set_xlabel('Confidence Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Per-class average confidence\n",
    "    avg_conf = [np.mean(class_conf[c]) if class_conf[c] else 0 for c in TARGET_CLASSES]\n",
    "    colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "    axes[1].bar(TARGET_CLASSES, avg_conf, color=colors, edgecolor='black', linewidth=0.8)\n",
    "    axes[1].set_title('Average Confidence Score per Class', fontweight='bold')\n",
    "    axes[1].set_xlabel('Class')\n",
    "    axes[1].set_ylabel('Average Confidence')\n",
    "    axes[1].set_ylim(0, 1.1)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    for i, (cls, val) in enumerate(zip(TARGET_CLASSES, avg_conf)):\n",
    "        axes[1].text(i, val + 0.02, f'{val:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "    plt.suptitle('Fine-tuned Model — Confidence Analysis', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\nelse:\n",
    "    print(\"No detections found for confidence analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Threshold and NMS Optimization\n",
    "\n",
    "We sweep confidence thresholds to analyze the precision-recall trade-off and find the optimal threshold for small object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep to analyze detection behavior\n",
    "thresholds = [0.25, 0.35, 0.45, 0.50, 0.60, 0.70]\n",
    "detection_counts = []\n",
    "\n",
    "print(f\"{'Threshold':<12} {'Total Detections':<20} {'Images w/ Detection':<22}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    results_t = finetuned_model.predict(\n",
    "        source=TEST_IMAGES_PATH,\n",
    "        conf=thresh,\n",
    "        verbose=False\n",
    "    )\n",
    "    total_d = sum(len(r.boxes) if r.boxes is not None else 0 for r in results_t)\n",
    "    imgs_d = sum(1 for r in results_t if r.boxes is not None and len(r.boxes) > 0)\n",
    "    detection_counts.append(total_d)\n",
    "    print(f\"{thresh:<12.2f} {total_d:<20} {imgs_d:<22}\")\n",
    "\n",
    "# Plot threshold vs detections\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(thresholds, detection_counts, marker='o', color='#2ecc71', linewidth=2, markersize=8)\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Default threshold (0.5)')\n",
    "plt.xlabel('Confidence Threshold', fontsize=11)\n",
    "plt.ylabel('Total Detections', fontsize=11)\n",
    "plt.title('Effect of Confidence Threshold on Detection Count', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Summary and Conclusions\n",
    "\n",
    "This section summarizes findings from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary printout\n",
    "print(\"=\" * 65)\n",
    "print(\" A3: SMALL OBJECT DETECTION — FINAL SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(\"Dataset: LISA Traffic Sign Dataset (via Roboflow)\")\n",
    "print(f\"Classes: {TARGET_CLASSES}\")\n",
    "print()\n",
    "print(\"── Pre-trained Model (YOLOv8n, COCO weights) ──────────────\")\n",
    "print(f\"  Cannot be evaluated with LISA class labels (different\")\n",
    "print(f\"  class set). Used as qualitative baseline only.\")\n",
    "print(f\"  Detections on test set: {total_detections}\")\n",
    "print()\n",
    "print(\"── Fine-tuned Model (YOLOv8n, LISA weights) ───────────────\")\n",
    "print(f\"  mAP@0.50:       {finetuned_metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@0.50:0.95:  {finetuned_metrics.box.map:.4f}\")\n",
    "print(f\"  Precision:      {finetuned_metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall:         {finetuned_metrics.box.mr:.4f}\")\n",
    "print(f\"  Detections on test set: {ft_total_detections}\")\n",
    "print()\n",
    "print(\"── Key Configurations for Small Object Detection ──────────\")\n",
    "print(\"  imgsz=1280   Higher resolution preserves small sign detail\")\n",
    "print(\"  augment=True Scale/blur/rotation augmentation\")\n",
    "print(\"  mosaic=1.0   Multi-scale object exposure\")\n",
    "print(\"  conf=0.50    Balanced precision-recall threshold\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Jocher, G. et al. (2023). *Ultralytics YOLOv8*. https://github.com/ultralytics/ultralytics  \n",
    "2. Mogelmose, A., Trivedi, M. M., & Moeslund, T. B. (2012). *Vision-based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey.* IEEE Transactions on Intelligent Transportation Systems.  \n",
    "3. LISA Traffic Sign Dataset. Roboflow Universe. https://universe.roboflow.com/lisatrafficlight/lisa-traffic  \n",
    "4. Redmon, J., & Farhadi, A. (2018). *YOLOv3: An Incremental Improvement.* arXiv:1804.02767."
   ]
  }
 ]
}
